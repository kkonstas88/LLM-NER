{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers.models.llama.modeling_llama import LlamaModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    # Open the file located at 'file_path' in read mode\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read the entire file content and remove any leading/trailing whitespace\n",
    "        content = f.read().strip() \n",
    "        # Split the content into sentences, where each sentence is separated by a blank line (\"\\n\\n\")\n",
    "        sentences = content.split(\"\\n\\n\")   \n",
    "        # Initialize an empty list to store the parsed data for each sentence\n",
    "        data = []   \n",
    "        # Iterate over each sentence in the list of sentences\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into individual tokens (each token is on a new line)\n",
    "            tokens = sentence.split(\"\\n\")    \n",
    "            # Initialize a list to store the data for each token in the current sentence\n",
    "            token_data = [] \n",
    "            # Iterate over each token in the sentence\n",
    "            for token in tokens:\n",
    "                # Split the token into its components (e.g., word, POS tag, etc.)\n",
    "                token_data.append(token.split())     \n",
    "            # Append the list of token data for the current sentence to the overall data list\n",
    "            data.append(token_data) \n",
    "    # Return the parsed data as a list of sentences, where each sentence is a list of tokens,\n",
    "    # and each token is represented as a list of its components (e.g., word, POS tag, etc.)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_data = read_conll_file(\"eng.train\")\n",
    "validation_data = read_conll_file(\"eng.testa\")\n",
    "test_data = read_conll_file(\"eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['-DOCSTART-', '-X-', '-X-', 'O']], [['EU', 'NNP', 'B-NP', 'B-ORG'], ['rejects', 'VBZ', 'B-VP', 'O'], ['German', 'JJ', 'B-NP', 'B-MISC'], ['call', 'NN', 'I-NP', 'O'], ['to', 'TO', 'B-VP', 'O'], ['boycott', 'VB', 'I-VP', 'O'], ['British', 'JJ', 'B-NP', 'B-MISC'], ['lamb', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "# Show example\n",
    "print(train_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(data, label_map):\n",
    "    # Initialize a dictionary to store formatted tokens and NER tags\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}   \n",
    "    # Iterate over each sentence in the input data\n",
    "    for sentence in data:\n",
    "        # Extract the tokens (first element of each token_data tuple) from the sentence\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        # Convert NER tags (fourth element of each token_data tuple) using label_map\n",
    "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
    "        # Append the extracted tokens and corresponding NER tags to the formatted_data dictionary\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    \n",
    "    # Convert the formatted data into a Dataset object and return it\n",
    "    return Dataset.from_dict(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique labels from the training data and sort them alphabetically\n",
    "label_list = sorted(\n",
    "    list(set([token_data[3] for sentence in train_data for token_data in sentence]))\n",
    ")\n",
    "\n",
    "# Create a mapping from each label to a unique integer index\n",
    "# This dictionary will map each label (from label_list) to its corresponding index\n",
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "# Print the list of labels\n",
    "print(label_list)\n",
    "\n",
    "# Print the mapping of labels to their corresponding indices or values\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary `id2label` by reversing the key-value pairs in `label_map`\n",
    "id2label = {value: key for key, value in label_map.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data into a dataset format using a label map for mapping labels\n",
    "train_dataset = convert_to_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_dataset(test_data, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DatasetDict object to hold multiple datasets\n",
    "datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": validation_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False     # Disables auto-tuning for convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create custom LlamaModel with bidirectional attention\n",
    "class LlamaBidirectionalModel(LlamaModel):\n",
    "    def _update_causal_mask(self, attention_mask):\n",
    "        # Create bidirectional attention mask (all ones)\n",
    "        bsz, seq_len = attention_mask.shape\n",
    "        mask = torch.ones((bsz, 1, seq_len, seq_len), dtype=torch.bool, device=attention_mask.device)\n",
    "        return mask\n",
    "\n",
    "# Initialize model with bidirectional attention\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label = id2label\n",
    ")\n",
    "\n",
    "# Replace the base model with bidirectional version\n",
    "model.base_model = LlamaBidirectionalModel(model.config)\n",
    "model.config.is_decoder = False # already bidirectional, but setting in any case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Verify model device placement\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Optional: Enable CUDA optimizations\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "def calculate_metrics(eval_pred):\n",
    "    # Handle both tuple and EvalPrediction object\n",
    "    if isinstance(eval_pred, tuple):\n",
    "        predictions, labels = eval_pred\n",
    "    else:\n",
    "        predictions = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    \n",
    "    # Get predictions by taking argmax\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with tuple: {'precision': 0.07142857142857142, 'recall': 0.08333333333333333, 'f1': 0.07692307692307691, 'accuracy': 0.1}\n",
      "Test with EvalPrediction: {'precision': 0.07142857142857142, 'recall': 0.08333333333333333, 'f1': 0.07692307692307691, 'accuracy': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Create mock data\n",
    "mock_predictions = np.random.rand(2, 10, len(label_list))  # Batch size 2, sequence length 10\n",
    "mock_labels = np.random.randint(0, len(label_list), size=(2, 10))\n",
    "\n",
    "# Test with tuple input\n",
    "test_metrics = calculate_metrics((mock_predictions, mock_labels))\n",
    "print(\"Test with tuple:\", test_metrics)\n",
    "\n",
    "# Test with EvalPrediction object\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "eval_pred = EvalPrediction(predictions=mock_predictions, label_ids=mock_labels)\n",
    "test_metrics = calculate_metrics(eval_pred)\n",
    "print(\"Test with EvalPrediction:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the input sentences, ensuring words are split properly\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each example's NER tags\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        # Align labels with word tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens like [CLS], [SEP], or padding tokens\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Assign label to the first token of a word\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # For subword tokens, assign -100 to ignore them during training\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # Add the aligned labels to the tokenized inputs\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    \n",
    "    # Optional: Debugging output to check alignment\n",
    "    print(f\"Tokenized Inputs: {tokenized_inputs['input_ids'][0]}\")\n",
    "    print(f\"Aligned Labels: {tokenized_inputs['labels'][0]}\")\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a8b5c70f7450699b57f2ab297a5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: [128000, 12, 32564, 23380, 12]\n",
      "Aligned Labels: [-100, 8, -100, -100, -100]\n",
      "Tokenized Inputs: [128000, 14202, 3791, 643, 12, 2279, 3711, 793, 97317, 3701, 6731, 34, 5152, 79923, 32002, 3711, 2871, 18846, 13]\n",
      "Aligned Labels: [-100, 8, -100, -100, 8, 2, -100, 8, -100, 8, -100, 8, -100, -100, 8, 8, -100, 8, 8]\n",
      "Tokenized Inputs: [128000, 44, 66933, 49, 56105, 12, 80101, 38, 22774, 8796, 4064, 47, 29286, 47, 51511, 5604, 35248, 1600, 13]\n",
      "Aligned Labels: [-100, 8, -100, 8, -100, 8, 1, -100, -100, 1, -100, 5, -100, 8, -100, -100, 8, -100, 8]\n",
      "Tokenized Inputs: [128000, 39, 7691, 42, 7691, 2550, 21, 12, 2318, 12, 1419]\n",
      "Aligned Labels: [-100, 0, -100, 4, -100, 8, -100, -100, -100, -100, -100]\n",
      "Tokenized Inputs: [128000, 20463, 35039, 25, 57, 41622, 276, 27368, 4845, 13]\n",
      "Aligned Labels: [-100, 8, 8, 8, 3, -100, -100, 7, -100, 8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb9f7a662654700a491e11aea748e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: [128000, 12, 32564, 23380, 12]\n",
      "Aligned Labels: [-100, 8, -100, -100, -100]\n",
      "Tokenized Inputs: [128000, 33, 32391, 1777, 790, 2550, 21, 12, 2318, 12, 966]\n",
      "Aligned Labels: [-100, 0, -100, -100, -100, 8, -100, -100, -100, -100, -100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def50d16751841659fdc201cb0cdfb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: [128000, 12, 32564, 23380, 12]\n",
      "Aligned Labels: [-100, 8, -100, -100, -100]\n",
      "Tokenized Inputs: [128000, 12, 32564, 23380, 12]\n",
      "Aligned Labels: [-100, 8, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "# # Apply the 'tokenize_and_align_labels' function to each example in the dataset using the map() method.\n",
    "# # - batched=True: This enables processing multiple examples at once (in batches) instead of one by one.\n",
    "# #   This speeds up the tokenization process, as tokenization libraries like Hugging Face's Tokenizers\n",
    "# #   can parallelize operations more efficiently when working with batches.\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['SOCCER',\n",
       "  '-',\n",
       "  'JAPAN',\n",
       "  'GET',\n",
       "  'LUCKY',\n",
       "  'WIN',\n",
       "  ',',\n",
       "  'CHINA',\n",
       "  'IN',\n",
       "  'SURPRISE',\n",
       "  'DEFEAT',\n",
       "  '.'],\n",
       " 'ner_tags': [8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"test\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['-DOCSTART-'], 'ner_tags': [8], 'input_ids': [128000, 12, 32564, 23380, 12], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [-100, 8, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
    "\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,     12,  32564,  23380,     12, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  14202,   3791,    643,     12,     41,   2599,   1111,   3891,\n",
      "          24115,   3096,     56,  24185,     11,   2198,  33004,    691,  92915,\n",
      "          58442,    937,   1170,  11673,    835,     13, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,     45,    329,    318,     43,    329,   6780, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,    984,   6830,    691,     11,  23175,   7098,    370,   2321,\n",
      "          25575,   2550,     21,     12,    717,     12,   2705, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  49852,     65,  16133,   1820,    755,    768,   1073,  50828,\n",
      "          68540,     34,    455,   2150,   4291,     64,     75,  10279,     17,\n",
      "             12,     16,   7678,  69849,  35767,   4298,    258,     64,   2878,\n",
      "             34,    331,  12520,   6481,    263,  35720,     13, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,   4071,  23078,     82,    675,  50828,  86476,   5919,    531,\n",
      "          49818,    258,   1820,   5686,   6481,   1073,   1820,   4166,     11,\n",
      "           5192,  19587,    998,     64,  20370,   9868,     17,     12,     15,\n",
      "            755,  33166,    998,    943,    884,    388,     52,  76113,   9121,\n",
      "             13, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  23078,  59707,   3646,   1073,   1820,   6481,    438,     82,\n",
      "            675,    325,  17998,    331,   3095,   1869,    291,  39830,   1820,\n",
      "           2495,    339,  43657,   9493,     52,  76113,  30972,   7197,     40,\n",
      "           5746,   2059,  44508,  11160,    258,  91453,  14625,  25843,   1073,\n",
      "             64,  34965,  20384,    291,    755,   4114,   2775,    998,   1718,\n",
      "           1820,   4047,   2017,   1820,  14625,   9151,  46023,  19393,    438,\n",
      "          18614,    276,   3274,   4816,     13],\n",
      "        [128000,     46,   1978,   2059,   1900,     74,  39342,  28010,  19643,\n",
      "           1073,   1820,   7678,    258,    258,  42861,   1712,     11,     71,\n",
      "          15154,    276,    359,    267,  78639,   2414,   5447,   6474,   1527,\n",
      "           4345,  68090,   1820,   4903,     13, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,    791,  35627,     50,    869,   3978,    265,    898,  16514,\n",
      "          43012,    258,    276,  68540,     34,    455,  12085,     82,  49831,\n",
      "           2000,   1820,   3983,   1712,     13, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  20397,   7678,   1251,   1820,  68540,  36907,   2150,  20375,\n",
      "          42820,   6438,     11,     52,  76113,   9121,    548,    258,   1820,\n",
      "          12085,     82,    300,  11934,  13572,     13, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  11874,  85257,   1527,    755,   4114,   7805,    258,   1820,\n",
      "           4354,  51464,  38137,  21642,  49852,    998,   2063,   1527,  30998,\n",
      "            485,    438,  17840,    543,  28956,   7862,   1527,  50828,  68785,\n",
      "          62349,  69849,  35767,   4298,     13, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000, 109848,  45644, 109848,  37135,  12618,     67,   1820,  53245,\n",
      "            258,   1820,   2421,    339,  43657,     11,  60023,    998,   2025,\n",
      "             64,     39,   8869,    939,   7404,     56,    276,    351,  97258,\n",
      "          29942,     83,  71839,   1820,  35767,   7414,  35039,   8370,  35039,\n",
      "          19393,     50,  22317,   8509,    277,  99567,    998,  19553,  21468,\n",
      "           8248,   3473,  21642,    998,   3306,    575,  18614,   1820,   4816,\n",
      "             13, 128001, 128001, 128001, 128001],\n",
      "        [128000,   2181,  16514,   1820,   5686,  16845,    398,   2067,   8154,\n",
      "           1729,  35767,   4298,    258,  35124,  38137,     13, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,   2685,   1693,     39,    395,    276,   5953,  18275,  25888,\n",
      "            998,   2295,   1512,     64,   4930,   4047,  18614,   1820,   4903,\n",
      "            258,   1820,   5833,    339,  43657,   8248,   3323,  26337,    998,\n",
      "            614,    531,    275,  18614,   1820,   3565,  74525,   1073,   8509,\n",
      "            277,    596,  35039,     13, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,     45,   1013,     41,    564,  32345,    277,  32345,  43068,\n",
      "          35767,   4298,   1820,  27152,   4291,     64,   9336,  42728,   1983,\n",
      "           2775,    258,   1820,    325,  45707,  43657,     13, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001],\n",
      "        [128000,  49852,   3473,     75,   3864,  79712,    713,    998,   1820,\n",
      "          35767,   7414,   2821,  10231,   4903,   2000,   3646,   1073,   1820,\n",
      "           5924,   8248,     81,    548,    398,  21152,   3939,   1820,  35767,\n",
      "           7414,    755,    768,     13, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8, -100, -100,    8,    0, -100, -100,    8,    8, -100, -100,\n",
      "            8,    8,    3, -100,    8,    8, -100, -100,    8, -100, -100,    8,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    3, -100, -100,    7, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0, -100, -100,    8,    0,    4, -100,    4, -100,    8, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    8, -100,    8,    8, -100,    8,    8,    1,    5, -100,\n",
      "            8,    8,    8,    8, -100,    8, -100, -100,    8,    8,    0, -100,\n",
      "            8,    8,    8,    8,    8, -100,    8,    8,    8,    8, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8,    0,    8, -100,    8,    8,    8, -100,    8,    8,    8,\n",
      "            8,    8,    8,    8,    8,    8,    8, -100,    8,    8,    8, -100,\n",
      "            8, -100, -100,    8, -100,    8,    8, -100, -100,    0, -100, -100,\n",
      "            8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    8,    8,    8,    8,    8,    8,    8, -100,    8, -100,\n",
      "            8, -100,    8, -100,    8,    8,    8, -100,    8,    8,    1, -100,\n",
      "            8, -100,    3, -100,    7, -100, -100, -100,    8,    8, -100,    8,\n",
      "            8,    8, -100, -100,    8, -100,    8,    8,    8,    8,    8,    8,\n",
      "            8,    8, -100,    1,    8,    8,    8,    8,    8,    8,    8],\n",
      "        [-100,    3, -100,    7, -100, -100, -100,    8,    8,    8,    8,    8,\n",
      "            8,    8, -100,    8,    8,    8, -100,    8,    8, -100, -100,    8,\n",
      "            8,    8,    8,    8,    8,    8,    8,    8, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8,    8,    1, -100, -100,    8, -100,    8,    8,    8,    8,\n",
      "            1,    5, -100,    8, -100,    8,    8,    8,    8,    8,    8, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8,    8, -100,    8,    1,    5,    8,    8,    8,    8,    8,\n",
      "            0, -100, -100,    8,    8,    8,    8, -100,    8,    8, -100,    8,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8,    8,    8,    8, -100,    8,    8,    8,    8,    8,    8,\n",
      "            8,    0,    8,    8,    8,    8, -100,    8,    8,    8,    8,    8,\n",
      "            8,    8,    8,    8,    8,    0, -100,    8, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    3, -100,    7, -100,    8, -100,    8,    8,    8,    8,    8,\n",
      "         -100,    8,    8,    8,    8,    8,    8,    3, -100, -100, -100,    7,\n",
      "         -100, -100, -100,    8,    8, -100,    8,    1, -100,    8,    8,    8,\n",
      "         -100,    3, -100,    7, -100,    8,    8,    8,    8,    8,    8,    8,\n",
      "            8,    8, -100,    8,    8,    8,    8, -100, -100, -100, -100],\n",
      "        [-100,    8,    8,    8,    8,    8, -100,    8, -100,    8,    0, -100,\n",
      "            8,    8,    8,    8, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    8, -100,    3, -100, -100,    7, -100,    8,    8,    8, -100,\n",
      "            8,    8,    8,    8,    8,    8,    8,    8,    8, -100,    8,    8,\n",
      "            8,    8,    8,    8, -100,    8,    8,    8,    8,    8,    8,    3,\n",
      "         -100,    8,    8,    8, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    3, -100,    7, -100, -100, -100,    8,    8,    0, -100,    8,\n",
      "            8,    8,    8,    8, -100, -100,    8,    8,    8,    8, -100,    8,\n",
      "            8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    8,    8, -100,    8, -100,    8,    8,    1, -100,    8,\n",
      "         -100,    8,    8,    8,    8,    8,    8,    8,    8, -100, -100,    8,\n",
      "         -100,    8,    1, -100,    8, -100,    8, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch = next(iter(DataLoader(tokenized_datasets[\"test\"], batch_size=16, collate_fn=data_collator)))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 14])\n",
      "Labels shape: torch.Size([3, 14])\n"
     ]
    }
   ],
   "source": [
    "test_batch = [\n",
    "    tokenized_datasets[\"train\"][i] for i in range(3)\n",
    "]\n",
    "\n",
    "# Test the collator\n",
    "collated = data_collator(test_batch)\n",
    "\n",
    "# Verify shape and content\n",
    "print(\"Input shape:\", collated[\"input_ids\"].shape)\n",
    "print(\"Labels shape:\", collated[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100],\n",
       "        [-100,    2,    8, -100,    1,    8,    8,    8, -100, -100,    1,    8,\n",
       "         -100,    8],\n",
       "        [-100,    3,    7, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_collator(collated_batch):\n",
    "    # Check if padding is applied correctly\n",
    "    assert torch.all(collated_batch[\"attention_mask\"][collated_batch[\"input_ids\"] == tokenizer.pad_token_id] == 0)\n",
    "    \n",
    "    # Verify label alignment\n",
    "    assert collated_batch[\"input_ids\"].shape == collated_batch[\"labels\"].shape\n",
    "    \n",
    "    return \"Validation passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique NER tags: {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "Model's number of labels: 9\n"
     ]
    }
   ],
   "source": [
    "# Check number of unique NER tags\n",
    "unique_labels = set([label for example in datasets['train'][\"ner_tags\"] for label in example])\n",
    "print(f\"Unique NER tags: {unique_labels}\")\n",
    "print(f\"Model's number of labels: {model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any label is outside the valid range\n",
    "for example in datasets['train'][\"ner_tags\"]:\n",
    "    for label in example:\n",
    "        if label < 0 or label >= model.config.num_labels:\n",
    "            raise ValueError(f\"Invalid label found: {label}. Expected range: [0, {model.config.num_labels - 1}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the context window explicitly to 8192 tokens\n",
    "# ctx_len = 8192\n",
    "# tokenizer.model_max_length = ctx_len\n",
    "# model.config.rope_freq_base = (ctx_len / 131_072) * 500_000\n",
    "# print(model.config.rope_freq_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 2,473,351,177 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],  # Only target attention layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"AutoModelForTokenClassification\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory where the model checkpoints and logs will be saved\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2e-4,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device (e.g., GPU)\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device (e.g., GPU)\n",
    "    num_train_epochs=1,  # Number of epochs to train the model\n",
    "    lr_scheduler_type=\"cosine\",  # Learning rate scheduler type (cosine annealing in this case)\n",
    "    remove_unused_columns=True,  # Keep all columns in the dataset, even if they are not used by the model\n",
    "    seed=42 # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer class for model training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to be trained\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # The tokenized training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # The tokenized validation dataset for evaluation during training\n",
    "    data_collator=data_collator,  # A function or object that batches and pads the data\n",
    "    processing_class=tokenizer,  # The tokenizer used for processing the input text\n",
    "    compute_metrics=calculate_metrics,  # A function to compute metrics during evaluation (e.g., accuracy, F1 score)\n",
    "    args=training_args  # Training arguments like batch size, number of epochs, learning rate, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1a4fef211f4bf4a90c8e1c651a84d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb2a0b77721438299a883aac3ee8ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 6.5292, 'eval_samples_per_second': 215.341, 'eval_steps_per_second': 13.478, 'epoch': 1.0}\n",
      "{'train_runtime': 52.7867, 'train_samples_per_second': 89.606, 'train_steps_per_second': 5.607, 'train_loss': 0.5196114101925412, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=296, training_loss=0.5196114101925412, metrics={'train_runtime': 52.7867, 'train_samples_per_second': 89.606, 'train_steps_per_second': 5.607, 'total_flos': 2875059165499092.0, 'train_loss': 0.5196114101925412, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('./bidirectional_llama32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tokenized test data from the preprocessed dataset\n",
    "test_dataset_tokenized = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e5fdf1c1e048b88018613ef1a688bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the trained model to perform predictions on the tokenized test dataset\n",
    "# 'predictions' will contain the model's output, 'labels' will contain the true labels,\n",
    "# and 'metrics' will store evaluation metrics (e.g., accuracy, precision, recall, etc.)\n",
    "predictions, labels, metrics = trainer.predict(test_dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_runtime': 12.041,\n",
       " 'test_samples_per_second': 135.37,\n",
       " 'test_steps_per_second': 8.471}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the evaluation metrics to assess the model's performance on the test set\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Steve Jobs, the co-founder of Apple Inc., was born in San Francisco, California.\n",
      "####\n",
      "Named Entities:\n",
      "Steve: B-ORG\n",
      "Apple: B-MISC\n",
      "Inc: B-MISC\n",
      "San: B-MISC\n",
      "Francisco: B-MISC\n",
      "California: B-MISC\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Steve Jobs, the co-founder of Apple Inc., was born in San Francisco, California.\"\n",
    "\n",
    "# Tokenize without adding special tokens\n",
    "tokenized_input = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(**tokenized_input)\n",
    "\n",
    "# Get predicted labels (argmax over logits)\n",
    "predicted_labels = outputs.logits.argmax(-1)[0]\n",
    "\n",
    "# Inverted label map (assuming label_map is defined elsewhere)\n",
    "label_map_inverted = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Initialize variables to store named entities\n",
    "named_entities = []\n",
    "current_entity_tokens = []\n",
    "current_label = None\n",
    "\n",
    "# Iterate over tokens and predicted labels\n",
    "for token_id, label_id in zip(tokenized_input[\"input_ids\"][0], predicted_labels):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    label = label_map_inverted[label_id.item()]\n",
    "\n",
    "    # Skip 'O' labels (non-entity tokens)\n",
    "    if label == \"O\":\n",
    "        if current_entity_tokens:\n",
    "            # Append the current entity and its label to the list\n",
    "            named_entities.append((\" \".join(current_entity_tokens).strip(), current_label))\n",
    "            current_entity_tokens = []\n",
    "            current_label = None\n",
    "        continue\n",
    "\n",
    "    # Handle subword tokens (tokens starting with '##')\n",
    "    if token.startswith(\"##\"):\n",
    "        current_entity_tokens[-1] += token[2:]  # Append subword to the last token\n",
    "    else:\n",
    "        # If it's a new entity or different from the current one, append the previous entity first\n",
    "        if not current_entity_tokens or label.split(\"-\")[0] == \"B\" or label != current_label:\n",
    "            if current_entity_tokens:\n",
    "                named_entities.append((\" \".join(current_entity_tokens).strip(), current_label))\n",
    "            current_entity_tokens = [token]  # Start a new entity\n",
    "        else:\n",
    "            current_entity_tokens.append(token)  # Continue appending to the current entity\n",
    "\n",
    "        current_label = label\n",
    "\n",
    "# Append any remaining entity at the end\n",
    "if current_entity_tokens:\n",
    "    named_entities.append((\" \".join(current_entity_tokens).strip(), current_label))\n",
    "\n",
    "# Print results\n",
    "print(\"Example 1:\", sentence)\n",
    "print(\"####\")\n",
    "print(\"Named Entities:\")\n",
    "for entity, label in named_entities:\n",
    "    print(f\"{entity}: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
