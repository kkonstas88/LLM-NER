{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from seqeval.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_conll_file(\"eng.train\")\n",
    "validation_data = read_conll_file(\"eng.testa\")\n",
    "test_data = read_conll_file(\"eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(data, label_map):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    return Dataset.from_dict(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(\n",
    "    list(set([token_data[3] for sentence in train_data for token_data in sentence]))\n",
    ")\n",
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "print(label_list)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_to_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_dataset(test_data, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": validation_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False     # Disables auto-tuning for convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Set padding token to be the same as EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Initialize model with updated tokenizer and move to GPU\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list))\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable causal mask for bidirectional attention\n",
    "model.config.is_decoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Verify model device placement\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Optional: Enable CUDA optimizations\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove special tokens\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_predictions, true_labels),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"classification_report\": classification_report(true_labels, true_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens are ignored during training\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b80e68f55e4de5aa8ee01fa15de076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e297b63f85e84c9c9cbbecc44fba6fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca8a09594814b04a807a73f9e341404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique NER tags: {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "Model's num_labels: 9\n"
     ]
    }
   ],
   "source": [
    "# Check number of unique NER tags\n",
    "unique_labels = set([label for example in datasets['train'][\"ner_tags\"] for label in example])\n",
    "print(f\"Unique NER tags: {unique_labels}\")\n",
    "print(f\"Model's num_labels: {model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any label is outside the valid range\n",
    "for example in datasets['train'][\"ner_tags\"]:\n",
    "    for label in example:\n",
    "        if label < 0 or label >= model.config.num_labels:\n",
    "            raise ValueError(f\"Invalid label found: {label}. Expected range: [0, {model.config.num_labels - 1}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the context window explicitly to 8192 tokens\n",
    "# ctx_len = 8192\n",
    "# tokenizer.model_max_length = ctx_len\n",
    "# model.config.rope_freq_base = (ctx_len / 131_072) * 500_000\n",
    "# print(model.config.rope_freq_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 1,237,536,777 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],  # Only target attention layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"AutoModelForTokenClassification\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2246d5948a7041bea695c9e6838d240e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3602, 'grad_norm': 1.2666654586791992, 'learning_rate': 0.00019859811225790162, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813475ab980448689b8a0f867cb316eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.4218, 'eval_samples_per_second': 188.147, 'eval_steps_per_second': 11.78, 'epoch': 1.0}\n",
      "{'loss': 0.1508, 'grad_norm': 0.88332200050354, 'learning_rate': 0.00019443175481643533, 'epoch': 1.07}\n",
      "{'loss': 0.1267, 'grad_norm': 0.6077597141265869, 'learning_rate': 0.00018761774298412903, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8584d41f33ac4e67a1c9107c5f60a661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.8502, 'eval_samples_per_second': 183.87, 'eval_steps_per_second': 11.512, 'epoch': 2.0}\n",
      "{'loss': 0.1183, 'grad_norm': 0.815678596496582, 'learning_rate': 0.00017834712635422716, 'epoch': 2.13}\n",
      "{'loss': 0.1023, 'grad_norm': 0.7046940326690674, 'learning_rate': 0.00016687983220303282, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9219b8b0813f4f168212659a9208b44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.7307, 'eval_samples_per_second': 185.044, 'eval_steps_per_second': 11.585, 'epoch': 3.0}\n",
      "{'loss': 0.0945, 'grad_norm': 0.2917778789997101, 'learning_rate': 0.00015353737771265787, 'epoch': 3.2}\n",
      "{'loss': 0.0878, 'grad_norm': 0.8045691251754761, 'learning_rate': 0.0001386938553510936, 'epoch': 3.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e39d0086246529f9f1aca04972ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.7722, 'eval_samples_per_second': 184.634, 'eval_steps_per_second': 11.56, 'epoch': 4.0}\n",
      "{'loss': 0.0802, 'grad_norm': 0.3310302793979645, 'learning_rate': 0.00012276544415930476, 'epoch': 4.27}\n",
      "{'loss': 0.0751, 'grad_norm': 0.2692268490791321, 'learning_rate': 0.00010619874102530885, 'epoch': 4.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1612c0d6591544848a24bcb2c4a6a498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.8673, 'eval_samples_per_second': 183.704, 'eval_steps_per_second': 11.501, 'epoch': 5.0}\n",
      "{'loss': 0.07, 'grad_norm': 0.542940080165863, 'learning_rate': 8.945823911011648e-05, 'epoch': 5.34}\n",
      "{'loss': 0.0648, 'grad_norm': 0.36107969284057617, 'learning_rate': 7.301330450235733e-05, 'epoch': 5.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e566a7eee7045ce8d37eb8838efb777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 19.4194, 'eval_samples_per_second': 178.482, 'eval_steps_per_second': 11.174, 'epoch': 6.0}\n",
      "{'loss': 0.0586, 'grad_norm': 0.24991106986999512, 'learning_rate': 5.73250162469559e-05, 'epoch': 6.4}\n",
      "{'loss': 0.057, 'grad_norm': 0.8931513428688049, 'learning_rate': 4.2833238723907275e-05, 'epoch': 6.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dea72727714f89bd29a3828c03fab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 19.3143, 'eval_samples_per_second': 179.453, 'eval_steps_per_second': 11.235, 'epoch': 7.0}\n",
      "{'loss': 0.0523, 'grad_norm': 0.9182114005088806, 'learning_rate': 2.9944288838627054e-05, 'epoch': 7.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b473867d03643868c71d3fa80ff0336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.9313, 'eval_samples_per_second': 183.083, 'eval_steps_per_second': 11.462, 'epoch': 8.0}\n",
      "{'loss': 0.0513, 'grad_norm': 0.5373820066452026, 'learning_rate': 1.9019543808169115e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0477, 'grad_norm': 0.4995848834514618, 'learning_rate': 1.0365308955408459e-05, 'epoch': 8.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2522412f6456476bb8958f82dde796d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.8594, 'eval_samples_per_second': 183.781, 'eval_steps_per_second': 11.506, 'epoch': 9.0}\n",
      "{'loss': 0.0486, 'grad_norm': 0.6393904685974121, 'learning_rate': 4.224229595491591e-06, 'epoch': 9.07}\n",
      "{'loss': 0.0459, 'grad_norm': 0.801257848739624, 'learning_rate': 7.684878059769363e-07, 'epoch': 9.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a13094249d4d03bf1eb08934ffb0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 18.913, 'eval_samples_per_second': 183.26, 'eval_steps_per_second': 11.474, 'epoch': 10.0}\n",
      "{'train_runtime': 2551.1713, 'train_samples_per_second': 58.746, 'train_steps_per_second': 3.673, 'train_loss': 0.09209455878910158, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9370, training_loss=0.09209455878910158, metrics={'train_runtime': 2551.1713, 'train_samples_per_second': 58.746, 'train_steps_per_second': 3.673, 'total_flos': 1.2430007246201301e+17, 'train_loss': 0.09209455878910158, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Steve Jobs, the co-founder of Apple Inc., was born in San Francisco, California.\n",
      "####\n",
      "Named Entities:\n",
      "Steve: I-MISC\n",
      " Jobs: B-PER\n",
      " Apple: B-ORG\n",
      " Inc: I-ORG\n",
      " Francisco: I-LOC\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "sentence = \"Steve Jobs, the co-founder of Apple Inc., was born in San Francisco, California.\"\n",
    "tokenized_input = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "outputs = model(**tokenized_input)\n",
    "predicted_labels = outputs.logits.argmax(-1)[0]\n",
    "\n",
    "label_map_inverted = {v: k for k, v in label_map.items()}\n",
    "\n",
    "named_entities = [\n",
    "    (tokenizer.decode([token]), label_map_inverted[label.item()])\n",
    "    for token, label in zip(tokenized_input[\"input_ids\"][0], predicted_labels)\n",
    "    if label != 0 and label != label_map[\"O\"]\n",
    "]\n",
    "\n",
    "print(\"Example 1: \" + sentence)\n",
    "print(\"####\")\n",
    "print(\"Named Entities:\")\n",
    "for entity, label in named_entities:\n",
    "    print(f\"{entity}: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
